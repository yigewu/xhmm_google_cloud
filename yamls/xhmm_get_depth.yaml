name: xhmm_get_depth
description:  Run GATK for depth of coverage

# Define the resources needed for this pipeline.
resources:
  zones:
  - us-west1-a

  # Create a data disk that is attached to the VM and destroyed when the
  # pipeline terminates.
  disks:
  - name: datadisk
    autoDelete: True

    # Within the Docker container, specify a mount point for the disk.
    mountPoint: /mnt/data

# Specify the Docker image to use along with the command
docker:
  imageName: yigewu/xhmm:google_test1

  # The Pipelines API will create the input directory when localizing files,
  # but does not create the output directory.
  cmd: >
    mkdir /mnt/data/output &&
    cd /mnt/data/input &&
    /usr/bin/java -jar /gatk3/GenomeAnalysisTK.jar -T DepthOfCoverage -I ${bamList} -L /mnt/data/input/GRCh37.ensGene.coding.exon.interval_list -R /mnt/data/input/GRCh37-lite.fa -dt BY_SAMPLE -dcov 5000 -l INFO --omitDepthOutputAtEachBase --omitLocusTable --minBaseQuality 0 --minMappingQuality 20 --start 1 --stop 5000 --nBins 200 --includeRefNSites --countType COUNT_FRAGMENTS -o /mnt/data/output/${bamBarcode}

inputParameters:
- name: bamFile
  description: the path for the BAM file
  localCopy:
    path: input/
    disk: datadisk
- name: baiFile
  description: the path for the BAM bai file
  localCopy:
    path: input/
    disk: datadisk
- name: bamList
  description: the name of the BAM file
- name: bamBarcode
  description: the barcode of the BAM file and the outputname of the coverage
- name: exonTargets
  description: the path for the example exon target file
  localCopy:
    path: input/
    disk: datadisk
- name: refFasta
  description: the path for the Human reference genome fasta file
  localCopy:
    path: input/
    disk: datadisk
- name: refIndex
  description: the path for the Human reference genome index file
  localCopy:
    path: input/
    disk: datadisk
- name: refDict
  description: the path for the Human reference genome dictionary file
  localCopy:
    path: input/
    disk: datadisk

# By specifying an outputParameter, we instruct the pipelines API to
# copy /mnt/data/output/* to the Cloud Storage location specified in
# the pipelineArgs (see below).
outputParameters:
- name: outputPath
  description: output path
  localCopy:
    path: output/*
    disk: datadisk
